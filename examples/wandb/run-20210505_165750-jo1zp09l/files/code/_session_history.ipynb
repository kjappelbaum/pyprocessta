{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dietary-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "import wandb\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, KFold\n",
    "import numpy as np \n",
    "\n",
    "MEAS_COLUMNS = [\n",
    "    \"TI-19\",\n",
    "    \"FI-16\",\n",
    "    \"TI-33\",\n",
    "    \"FI-2\",\n",
    "    \"FI-151\",\n",
    "    \"TI-8\",\n",
    "    \"FI-241\",\n",
    "#   \"valve-position-12\",  # dry-bed\n",
    "#     \"FI-38\",  # strippera\n",
    "#     \"PI-28\",  # stripper\n",
    "    \n",
    "#     \"TI-28\",  # stripper\n",
    "    \"FI-20\",\n",
    "    \"FI-30\",\n",
    "    \"TI-3\",\n",
    "    \"FI-19\",\n",
    "    \"FI-211\",\n",
    "    \"FI-11\",\n",
    "    \"TI-30\",\n",
    "    \"PI-30\",\n",
    "    \"TI-1213\",\n",
    "#     \"TI-4\",\n",
    "    \"FI-23\",\n",
    "    \"delta_t\",\n",
    "]\n",
    "\n",
    "TARGETS_clean = ['2-Amino-2-methylpropanol C4H11NO', 'Piperazine C4H10N2', \n",
    "                 \"Carbon dioxide CO2\", \"Ammonia NH3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confused-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df_dropped.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "chronic-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[MEAS_COLUMNS], df[TARGETS_clean].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "another-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "current-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nervous-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_estimators': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 10,\n",
    "        'max': 5000\n",
    "    },\n",
    "    'max_depth': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 100\n",
    "    },\n",
    "    'num_leaves': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 500\n",
    "    },\n",
    "    'reg_alpha': {\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.4\n",
    "    },\n",
    "    'reg_lambda': {\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.4\n",
    "    },\n",
    "    'subsample': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.4,\n",
    "        'max': 1.0\n",
    "    },\n",
    "    'colsample_bytree': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.01,\n",
    "        'max': 1.0\n",
    "    },\n",
    "    'min_child_weight': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.001,\n",
    "        'max': 0.1,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organic-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_id(method):\n",
    "    \"\"\"return us a sweep id (required for running the sweep)\"\"\"\n",
    "    sweep_config = {\n",
    "        'method': method,\n",
    "        'metric': {\n",
    "            'name': 'cv_mean',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'early_terminate': {\n",
    "            'type': 'hyperband',\n",
    "            's': 2,\n",
    "            'eta': 3,\n",
    "            'max_iter': 30\n",
    "        },\n",
    "        'parameters': config,\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project='process_ml')\n",
    "\n",
    "    return sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inner-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "\n",
    "    configs = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 50,\n",
    "        'reg_alpha': 0.00001,\n",
    "        'reg_lambda': 0.00001,\n",
    "        'subsample': 0.2,\n",
    "        'colsample_bytree': 0.2,\n",
    "        'min_child_weight': 0.001,\n",
    "    }\n",
    "\n",
    "    # Initilize a new wandb run\n",
    "    wandb.init(project='process_ml', config=configs)\n",
    "\n",
    "    config = wandb.config\n",
    "    config['objective'] =  'huber'\n",
    "\n",
    "    regressor = LGBMRegressor(**config)\n",
    "\n",
    "    cv = cross_val_score(regressor, X_, y[:, index], n_jobs=-1, cv=KFold(n_splits=5), scoring='neg_mean_absolute_error')\n",
    "\n",
    "    mean = np.abs(cv.mean())\n",
    "    std = np.abs(cv.std())\n",
    "    wandb.log({'cv_mean': mean})\n",
    "    wandb.log({'cv_std': std})\n",
    "\n",
    "    wandb.run.summary['cv_mean'] = mean\n",
    "    wandb.run.summary['cv_std'] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accurate-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = get_sweep_id('bayes')\n",
    "train_func = partial(train, index=int(1))\n",
    "wandb.agent(sweep_id, function=train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "social-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = get_sweep_id('bayes')\n",
    "train_func = partial(train, index=int(2))\n",
    "wandb.agent(sweep_id, function=train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "established-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_0_config = { # https://wandb.ai/kjappelbaum/process_ml/runs/c0g28ueb/overview?workspace=user-kjappelbaum\n",
    "    'n_estimators': 1428,\n",
    "    'max_depth': 70,\n",
    "    'num_leaves': 5,\n",
    "    'reg_alpha': 1.382,\n",
    "    'reg_lambda': 1.26,\n",
    "    'subsample': 0.8367,\n",
    "    'colsample_bytree': 0.522,\n",
    "    'min_child_weight': 0.09979,\n",
    "}\n",
    "\n",
    "target_1_config = { \n",
    "    'n_estimators': 261,\n",
    "    'max_depth': 88,\n",
    "    'num_leaves': 272,\n",
    "    'reg_alpha': 1.071,\n",
    "    'reg_lambda': 1.036,\n",
    "    'subsample': 0.4096,\n",
    "    'colsample_bytree': 0.1534,\n",
    "    'min_child_weight': 0.0117,\n",
    "   # 'objective': 'huber'\n",
    "}\n",
    "\n",
    "\n",
    "target_2_config = { # https://wandb.ai/kjappelbaum/process_ml/runs/571rdxdm/overview?workspace=user-kjappelbaum\n",
    "    'n_estimators': 228,\n",
    "    'max_depth': 39,\n",
    "    'num_leaves': 440,\n",
    "    'reg_alpha': 1.456,\n",
    "    'reg_lambda': 1.415,\n",
    "    'subsample': 0.4801,\n",
    "    'colsample_bytree': 0.3772,\n",
    "    'min_child_weight': 0.01326,\n",
    "    #'objective': 'huber'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "knowing-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lined-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_co2 = BaggingRegressor(LGBMRegressor(**target_0_config), n_estimators=100)\n",
    "xgb_2amp = BaggingRegressor(LGBMRegressor(**target_1_config), n_estimators=100)\n",
    "xgb_piperazine = BaggingRegressor(LGBMRegressor(**target_2_config), n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suburban-billion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "silent-official",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingRegressor(base_estimator=LGBMRegressor(colsample_bytree=0.3772,\n",
      "                                              max_depth=39,\n",
      "                                              min_child_weight=0.01326,\n",
      "                                              n_estimators=228, num_leaves=440,\n",
      "                                              reg_alpha=1.456, reg_lambda=1.415,\n",
      "                                              subsample=0.4801),\n",
      "                 n_estimators=100)"
     ]
    }
   ],
   "source": [
    "xgb_co2.fit(X_train, y_train[:, 0])\n",
    "xgb_2amp.fit(X_train, y_train[:, 1])\n",
    "xgb_piperazine.fit(X_train, y_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "subjective-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parity_plot(model, X_test, X_train, y_test_, y_train_, outname=None):\n",
    "    predictions_test = model.predict(X_test)\n",
    "    predictions_train = model.predict(X_train)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].set_ylabel(r'$\\hat{y}$')\n",
    "    ax[0].set_title('test')\n",
    "    ax[1].set_title('train')\n",
    "\n",
    "    ax[0].scatter(y_test_, predictions_test, s=10)\n",
    "    ax[1].scatter(y_train_, predictions_train, s=10)\n",
    "\n",
    "    for a in ax: \n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.spines['left'].set_smart_bounds(True)\n",
    "        a.spines['bottom'].set_smart_bounds(True)\n",
    "        a.set_xlabel(r'$y_\\mathrm{true}$')\n",
    "\n",
    "        x_lims = a.get_xlim()\n",
    "        y_lims = a.get_ylim()\n",
    "        a.plot(x_lims, y_lims, '--k')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if outname is not None: \n",
    "        fig.savefig(outname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "artistic-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_co2, X_test, X_train, y_test[:,0], y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "royal-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_2amp, X_test, X_train, y_test[:,1], y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secret-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_piperazine, X_test, X_train, y_test[:,2], y_train[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "twenty-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parity_plot(model, X_test, X_train, y_test_, y_train_, outname=None):\n",
    "    predictions_test = model.predict(X_test)\n",
    "    predictions_train = model.predict(X_train)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].set_ylabel(r'$\\hat{y}$')\n",
    "    ax[0].set_title('test')\n",
    "    ax[1].set_title('train')\n",
    "\n",
    "    ax[0].scatter(y_test_, predictions_test, s=10)\n",
    "    ax[1].scatter(y_train_, predictions_train, s=10)\n",
    "\n",
    "    for a in ax: \n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.spines['left'].set_smart_bounds(True)\n",
    "        a.spines['bottom'].set_smart_bounds(True)\n",
    "        a.set_xlabel(r'$y_\\mathrm{true}$')\n",
    "\n",
    "        x_lims = a.get_xlim()\n",
    "        y_lims = a.get_ylim()\n",
    "        a.plot(x_lims, y_lims, '--k', s=1)\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if outname is not None: \n",
    "        fig.savefig(outname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "radio-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_co2, X_test, X_train, y_test[:,0], y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "therapeutic-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parity_plot(model, X_test, X_train, y_test_, y_train_, outname=None):\n",
    "    predictions_test = model.predict(X_test)\n",
    "    predictions_train = model.predict(X_train)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].set_ylabel(r'$\\hat{y}$')\n",
    "    ax[0].set_title('test')\n",
    "    ax[1].set_title('train')\n",
    "\n",
    "    ax[0].scatter(y_test_, predictions_test, s=2)\n",
    "    ax[1].scatter(y_train_, predictions_train, s=2)\n",
    "\n",
    "    for a in ax: \n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.spines['left'].set_smart_bounds(True)\n",
    "        a.spines['bottom'].set_smart_bounds(True)\n",
    "        a.set_xlabel(r'$y_\\mathrm{true}$')\n",
    "\n",
    "        x_lims = a.get_xlim()\n",
    "        y_lims = a.get_ylim()\n",
    "        a.plot(x_lims, y_lims, '--k')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if outname is not None: \n",
    "        fig.savefig(outname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "closing-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_co2, X_test, X_train, y_test[:,0], y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "czech-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_0_config = { # https://wandb.ai/kjappelbaum/process_ml/runs/c0g28ueb/overview?workspace=user-kjappelbaum\n",
    "    'n_estimators': 1428,\n",
    "    'max_depth': 70,\n",
    "    'num_leaves': 5,\n",
    "    'reg_alpha': 1.382,\n",
    "    'reg_lambda': 1.26,\n",
    "    'subsample': 0.8367,\n",
    "    'colsample_bytree': 0.522,\n",
    "    'min_child_weight': 0.09979,\n",
    "     'objective': 'huber'\n",
    "}\n",
    "\n",
    "target_1_config = { \n",
    "    'n_estimators': 261,\n",
    "    'max_depth': 88,\n",
    "    'num_leaves': 272,\n",
    "    'reg_alpha': 1.071,\n",
    "    'reg_lambda': 1.036,\n",
    "    'subsample': 0.4096,\n",
    "    'colsample_bytree': 0.1534,\n",
    "    'min_child_weight': 0.0117,\n",
    "    'objective': 'huber'\n",
    "}\n",
    "\n",
    "\n",
    "target_2_config = { # https://wandb.ai/kjappelbaum/process_ml/runs/571rdxdm/overview?workspace=user-kjappelbaum\n",
    "    'n_estimators': 228,\n",
    "    'max_depth': 39,\n",
    "    'num_leaves': 440,\n",
    "    'reg_alpha': 1.456,\n",
    "    'reg_lambda': 1.415,\n",
    "    'subsample': 0.4801,\n",
    "    'colsample_bytree': 0.3772,\n",
    "    'min_child_weight': 0.01326,\n",
    "    'objective': 'huber'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "damaged-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "friendly-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_co2 = BaggingRegressor(LGBMRegressor(**target_0_config), n_estimators=100)\n",
    "xgb_2amp = BaggingRegressor(LGBMRegressor(**target_1_config), n_estimators=100)\n",
    "xgb_piperazine = BaggingRegressor(LGBMRegressor(**target_2_config), n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "existing-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "continental-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingRegressor(base_estimator=LGBMRegressor(colsample_bytree=0.3772,\n",
      "                                              max_depth=39,\n",
      "                                              min_child_weight=0.01326,\n",
      "                                              n_estimators=228, num_leaves=440,\n",
      "                                              objective='huber',\n",
      "                                              reg_alpha=1.456, reg_lambda=1.415,\n",
      "                                              subsample=0.4801),\n",
      "                 n_estimators=100)"
     ]
    }
   ],
   "source": [
    "xgb_co2.fit(X_train, y_train[:, 0])\n",
    "xgb_2amp.fit(X_train, y_train[:, 1])\n",
    "xgb_piperazine.fit(X_train, y_train[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "favorite-positive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parity_plot(model, X_test, X_train, y_test_, y_train_, outname=None):\n",
    "    predictions_test = model.predict(X_test)\n",
    "    predictions_train = model.predict(X_train)\n",
    "\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].set_ylabel(r'$\\hat{y}$')\n",
    "    ax[0].set_title('test')\n",
    "    ax[1].set_title('train')\n",
    "\n",
    "    ax[0].scatter(y_test_, predictions_test, s=2)\n",
    "    ax[1].scatter(y_train_, predictions_train, s=2)\n",
    "\n",
    "    for a in ax: \n",
    "        a.spines['top'].set_color('none')\n",
    "        a.spines['right'].set_color('none')\n",
    "        a.spines['left'].set_smart_bounds(True)\n",
    "        a.spines['bottom'].set_smart_bounds(True)\n",
    "        a.set_xlabel(r'$y_\\mathrm{true}$')\n",
    "\n",
    "        x_lims = a.get_xlim()\n",
    "        y_lims = a.get_ylim()\n",
    "        a.plot(x_lims, y_lims, '--k')\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if outname is not None: \n",
    "        fig.savefig(outname, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "scientific-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_co2, X_test, X_train, y_test[:,0], y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "renewable-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_2amp, X_test, X_train, y_test[:,1], y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hindu-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_piperazine, X_test, X_train, y_test[:,2], y_train[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "refined-moore",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_co2, X_test, X_train, y_test[:,0], y_train[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "divine-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_2amp, X_test, X_train, y_test[:,1], y_train[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "intellectual-berkeley",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_parity_plot(xgb_piperazine, X_test, X_train, y_test[:,2], y_train[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "impaired-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "\n",
    "    configs = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 50,\n",
    "        'reg_alpha': 0.00001,\n",
    "        'reg_lambda': 0.00001,\n",
    "        'subsample': 0.2,\n",
    "        'colsample_bytree': 0.2,\n",
    "        'min_child_weight': 0.001,\n",
    "    }\n",
    "\n",
    "    # Initilize a new wandb run\n",
    "    wandb.init(project='process_ml', config=configs)\n",
    "\n",
    "    config = wandb.config\n",
    "    #config['objective'] =  'huber'\n",
    "\n",
    "    regressor = LGBMRegressor(**config)\n",
    "\n",
    "    cv = cross_val_score(regressor, X_, y[:, index], n_jobs=-1, cv=KFold(n_splits=5), scoring='neg_mean_absolute_error')\n",
    "\n",
    "    mean = np.abs(cv.mean())\n",
    "    std = np.abs(cv.std())\n",
    "    wandb.log({'cv_mean': mean})\n",
    "    wandb.log({'cv_std': std})\n",
    "\n",
    "    wandb.run.summary['cv_mean'] = mean\n",
    "    wandb.run.summary['cv_std'] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "trained-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_estimators': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 10,\n",
    "        'max': 1000\n",
    "    },\n",
    "    'max_depth': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 100\n",
    "    },\n",
    "    'num_leaves': {\n",
    "        'distribution': 'int_uniform',\n",
    "        'min': 5,\n",
    "        'max': 500\n",
    "    },\n",
    "    'reg_alpha': {\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.4\n",
    "    },\n",
    "    'reg_lambda': {\n",
    "        'distribution': 'log_uniform',\n",
    "        'min': 0.00001,\n",
    "        'max': 0.4\n",
    "    },\n",
    "    'subsample': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.4,\n",
    "        'max': 1.0\n",
    "    },\n",
    "    'colsample_bytree': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.4,\n",
    "        'max': 1.0\n",
    "    },\n",
    "    'min_child_weight': {\n",
    "        'distribution': 'uniform',\n",
    "        'min': 0.0001,\n",
    "        'max': 0.1,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "vertical-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_id(method):\n",
    "    \"\"\"return us a sweep id (required for running the sweep)\"\"\"\n",
    "    sweep_config = {\n",
    "        'method': method,\n",
    "        'metric': {\n",
    "            'name': 'cv_mean',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'early_terminate': {\n",
    "            'type': 'hyperband',\n",
    "            's': 2,\n",
    "            'eta': 3,\n",
    "            'max_iter': 30\n",
    "        },\n",
    "        'parameters': config,\n",
    "    }\n",
    "    sweep_id = wandb.sweep(sweep_config, project='process_ml')\n",
    "\n",
    "    return sweep_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "above-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    # Config is a variable that holds and saves hyperparameters and inputs\n",
    "\n",
    "    configs = {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 50,\n",
    "        'reg_alpha': 0.00001,\n",
    "        'reg_lambda': 0.00001,\n",
    "        'subsample': 0.2,\n",
    "        'colsample_bytree': 0.2,\n",
    "        'min_child_weight': 0.001,\n",
    "    }\n",
    "\n",
    "    # Initilize a new wandb run\n",
    "    wandb.init(project='process_ml', config=configs)\n",
    "\n",
    "    config = wandb.config\n",
    "    #config['objective'] =  'huber'\n",
    "\n",
    "    regressor = LGBMRegressor(**config)\n",
    "\n",
    "    cv = cross_val_score(regressor, X_, y[:, index], n_jobs=-1, cv=KFold(n_splits=5), scoring='neg_mean_absolute_error')\n",
    "\n",
    "    mean = np.abs(cv.mean())\n",
    "    std = np.abs(cv.std())\n",
    "    wandb.log({'cv_mean': mean})\n",
    "    wandb.log({'cv_std': std})\n",
    "\n",
    "    wandb.run.summary['cv_mean'] = mean\n",
    "    wandb.run.summary['cv_std'] = std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "civilian-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.28<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">fearless-sweep-4</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/kjappelbaum/process_ml\" target=\"_blank\">https://wandb.ai/kjappelbaum/process_ml</a><br/>\n",
       "                Sweep page: <a href=\"https://wandb.ai/kjappelbaum/process_ml/sweeps/iiva5ijf\" target=\"_blank\">https://wandb.ai/kjappelbaum/process_ml/sweeps/iiva5ijf</a><br/>\n",
       "Run page: <a href=\"https://wandb.ai/kjappelbaum/process_ml/runs/jo1zp09l\" target=\"_blank\">https://wandb.ai/kjappelbaum/process_ml/runs/jo1zp09l</a><br/>\n",
       "                Run data is saved locally in <code>/Users/kevinmaikjablonka/Dropbox (LSMO)/Documents/open_source/pyprocessta/examples/wandb/run-20210505_165750-jo1zp09l</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = get_sweep_id('bayes')\n",
    "train_func = partial(train, index=int(0))\n",
    "wandb.agent(sweep_id, function=train_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
